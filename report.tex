\documentclass{article}

\usepackage{hcmut-report}
\usepackage{codespace}

% Sub-preambles
% https://github.com/MartinScharrer/standalone

% Encodings
\usepackage{amsmath,amssymb,gensymb,textcomp}

% Better tables
% Wide tables go to https://tex.stackexchange.com/q/332902
\usepackage{array,multicol,multirow,siunitx,tabularx}

% Better enum
\usepackage{enumitem}

% Graphics
\usepackage{caption,float}

% Allow setting >max< width of figure
% Remove if unneeded
\usepackage[export]{adjustbox} % 'export' allows adjustbox keys in \includegraphics
\usepackage{mwe}

% Code highlighting
\usepackage{minted}
\usemintedstyle{friendly}

% Custom commands
% Remove if unneeded
\newcommand*\mean[1]{\bar{#1}}

\ocoursename{\huge Logic Design Project}
\title{\large Camera Data Acquisition and HDMI Output on FPGA}
\oadvisor{MSc. Huynh Phuc Nghi}
\ostudent{Nguyễn Trương Đức Tài | 2252723}
\reportlayout%

\begin{document}

\coverpage%

\newpage
\tableofcontents
\newpage

\begin{abstract}
This report details the design and implementation of a real-time video processing system on the Digilent Arty Z7 FPGA platform. The project aims to demonstrate the capabilities of heterogeneous System-on-Chip (SoC) architectures in edge computing applications. The core objective was to establish a high-performance video pipeline capable of capturing high-definition video from a USB camera, processing it using computer vision algorithms, and outputting the result to an HDMI display with minimal latency.

The final system successfully implements a multi-threaded pipeline using the PYNQ framework, achieving stable 720p streaming at 30 FPS and 1080p streaming at 24-26 FPS. A lightweight motion detection algorithm based on frame differencing was integrated, demonstrating the platform's ability to handle real-world computer vision tasks. This report covers the hardware architecture, software design, algorithmic implementation, and performance analysis, providing a comprehensive guide to reproducing and extending the work.
\end{abstract}

\section{Introduction}

\subsection{Project Context}
This project explores the potential of edge AI and embedded vision using the \textbf{Arty Z7} platform. In an era where smart cameras and IoT devices are ubiquitous, the ability to process video data locally at the "edge" is critical for reducing bandwidth usage, ensuring privacy, and minimizing latency. The Arty Z7, with its combination of ARM Cortex-A9 processors and Artix-7 FPGA fabric, represents an ideal platform for prototyping such systems.

Unlike traditional desktop computer vision setups that rely on powerful GPUs and unlimited power budgets, embedded vision requires a careful balance of performance and efficiency. The Zynq architecture allows for a unique "Hardware-Software Co-design" approach, where computationally intensive tasks can be offloaded to the FPGA logic while complex control flows are handled by the ARM processor.

This project focuses on the fundamental building block of any vision system: the video pipeline. Before complex AI models can be deployed, a robust system must be in place to acquire, buffer, process, and display video frames reliably. This report documents the journey of building that foundation and extending it with functional motion detection capabilities.

\textbf{Note on Camera Selection:} Originally, I intended to utilize the OV7670 camera module via the PMOD interface. However, since I had a high-quality USB webcam readily available, I adapted the design to leverage the Zynq's USB Host capabilities. I found that this approach not only simplified the physical connections but also demonstrated the flexibility of the PYNQ framework in handling standard UVC devices, making the system more versatile for real-world applications.

\subsection{Problem Statement}
Developing for embedded hardware like the Arty Z7 presents distinct challenges compared to standard desktop environments. The ARM Cortex-A9 processor has limited computational power, and system memory is constrained. A primary difficulty lies in ensuring the video pipeline operates smoothly at high resolutions (720p/1080p) without frame drops or perceptible latency, particularly when introducing the additional computational load of motion detection.

The core challenge was to design a software architecture that maximizes throughput without exhausting these resources, ensuring smooth video playback while maintaining sufficient headroom for image processing algorithms.

\subsection{Project Objectives}
The project was guided by a set of core requirements and ambitious bonus objectives:

\textbf{Core Requirements:}
\begin{enumerate}
    \item \textbf{USB Camera Interface:} Successfully interface with a standard UVC (USB Video Class) webcam using standard Linux drivers.
    \item \textbf{HDMI Output:} Drive an HDMI monitor directly from the FPGA board using the programmable logic video controller.
    \item \textbf{Pass-through Streaming:} Create a pipeline to display the live camera feed on the monitor with minimal latency.
\end{enumerate}

\textbf{Bonus Objectives (Achieved):}
\begin{enumerate}
    \item \textbf{High Resolution:} Target 720p (1280x720) and 1080p (1920x1080) resolutions.
    \item \textbf{Computer Vision Integration:} Implement a real-time motion detection algorithm.
    \item \textbf{Frame Storage:} Design the architecture to support frame capture and storage (implemented in logic).
    \item \textbf{SoC Utilization:} Leverage the Zynq architecture effectively, utilizing both PS and PL components.
\end{enumerate}

\textbf{Future Enhancements:}
\begin{enumerate}
    \item \textbf{RISC-V Integration:} Explore the integration of soft-core processors (PicoRV32/Ibex) for auxiliary tasks.
\end{enumerate}

\section{Hardware Architecture}

\subsection{The Arty Z7 Platform}
The Digilent Arty Z7-20 is the heart of this project. It features the Xilinx Zynq-7000 XC7Z020-1CLG400C SoC, which integrates a dual-core ARM Cortex-A9 Processing System (PS) with Artix-7 Programmable Logic (PL).

\textbf{Key Specifications:}
\begin{itemize}
    \item \textbf{Processor:} Dual-core ARM Cortex-A9 @ 650 MHz.
    \item \textbf{FPGA Logic:} 85,000 logic cells, 53,200 LUTs, 106,400 flip-flops.
    \item \textbf{Memory:} 512MB DDR3 with 16-bit bus @ 1050 Mbps.
    \item \textbf{Video Output:} HDMI Sink and Source ports (I utilize the Source/Output).
    \item \textbf{Connectivity:} USB 2.0 Host (for camera), Gigabit Ethernet, UART.
\end{itemize}

This heterogeneous architecture allows me to run a full Linux operating system on the PS while offloading high-speed I/O and timing-critical video tasks to the PL. The PS and PL communicate via high-performance AXI (Advanced eXtensible Interface) ports, allowing the FPGA logic to access the main system memory directly.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/arty_z7_photo.jpg}
  \caption{The Digilent Arty Z7-20 FPGA Development Board.}
  \label{fig:arty_z7}
\end{figure}

\subsection{Peripheral Setup}
The system interacts with two primary external peripherals:

\begin{enumerate}
    \item \textbf{Input: Ugreen 2K Webcam}
    \begin{itemize}
        \item \textbf{Interface:} USB 2.0.
        \item \textbf{Max Resolution:} 2560x1440 (2K).
        \item \textbf{Formats:} MJPEG (Compressed), YUYV (Raw).
        \item \textbf{Role:} The camera acts as the data source. I utilize the MJPEG format to minimize USB bandwidth usage. Raw YUYV at 1080p would require approx 3 Gbps ($1920 \times 1080 \times 16 \text{ bits} \times 30 \text{ FPS}$), which far exceeds the 480 Mbps limit of USB 2.0. MJPEG compression brings this within range.
    \end{itemize}

    \item \textbf{Output: HDMI Monitor}
    \begin{itemize}
        \item \textbf{Interface:} HDMI Type A.
        \item \textbf{Resolution:} Supports standard 720p60 and 1080p60 timings.
        \item \textbf{Role:} Displays the processed video feed and the On-Screen Display (OSD) overlay containing performance metrics and detection bounding boxes.
    \end{itemize}
\end{enumerate}

\subsection{Hardware Setup and Configuration}
To establish the experimental environment, the Arty Z7 board was configured and connected as detailed in Figure \ref{fig:arty_diagram}. The setup process involved system initialization, physical interfacing, and network configuration.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{assets/arty_z7_diagram.png}
  \caption{Detailed Diagram of the Arty Z7 Components and Interfaces.}
  \label{fig:arty_diagram}
\end{figure}

\subsubsection{1. System Initialization}
The PYNQ-Z1 disk image was selected for this project due to its compatibility with the Zynq 7020 SoC found on the Arty Z7-20. A 16GB Class 10 MicroSD card was flashed with this image using BalenaEtcher, ensuring a reliable root filesystem for the embedded Linux environment.

\subsubsection{2. Hardware Interface}
The physical connections were established as follows:
\begin{enumerate}
    \item \textbf{Boot Mode Configuration:} Jumper \textbf{JP4} was set to the \textbf{SD} position to enable booting from the MicroSD card.
    \item \textbf{Storage:} The prepared MicroSD card was inserted into the board's slot.
    \item \textbf{Control and Power:} A MicroUSB cable connected the laptop to the \textbf{PROG UART} port (J14), providing both power and a serial console interface.
    \item \textbf{Video Output:} The HDMI monitor was connected to the \textbf{HDMI OUT} port.
    \item \textbf{Network Interface:} A direct Ethernet connection was established between the Arty Z7 and the host laptop to ensure low-latency communication.
\end{enumerate}

\subsubsection{3. Boot Sequence Verification}
Upon powering the board via switch \textbf{SW0}, the boot sequence was verified through the on-board LEDs:
\begin{itemize}
    \item \textbf{Power Status:} The Red LED (LD13) illuminated immediately.
    \item \textbf{FPGA Configuration:} The "DONE" LED (LD12) lit up, confirming the successful loading of the bitstream.
    \item \textbf{Kernel Boot:} The RGB LEDs (LD0-LD3) flashed during the Linux kernel initialization, stabilizing after approximately two minutes.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/arty_leds.jpg}
  \caption{Arty Z7 LED Indicators during Boot (LD0-LD3 and LD10-LD13).}
  \label{fig:arty_leds}
\end{figure}

\subsubsection{4. Network Configuration and Access}
To enable communication over the direct Ethernet link, the host laptop's network adapter was configured with a static IP address, as the board defaults to a static IP of \texttt{192.168.2.99}.

\textbf{Host Configuration:}
\begin{itemize}
    \item \textbf{IP Address:} \texttt{192.168.2.1}
    \item \textbf{Subnet Mask:} \texttt{255.255.255.0}
\end{itemize}

Once configured, the system was accessed via the Jupyter Notebook interface by navigating to \url{http://192.168.2.99} in a web browser. This provided the primary development environment for executing Python code and visualizing results.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/setup_jupyter_login.png}
  \caption{Jupyter Notebook Login Screen on the Arty Z7.}
  \label{fig:jupyter_login}
\end{figure}
\end{itemize}

\subsection{Programmable Logic (PL) Design}
The hardware design, created in Xilinx Vivado, provides the necessary infrastructure to drive the HDMI output. While the image processing is currently performed in software on the PS, the PL handles the critical video timing and physical interface.

\textbf{Key Hardware Blocks:}
\begin{itemize}
    \item \textbf{Zynq Processing System (IP):} The interface between the ARM cores and the FPGA fabric. It provides AXI ports for data transfer.
    \item \textbf{AXI VDMA (Video Direct Memory Access):} A crucial component that reads video frames from the DDR3 memory and streams them to the video output pipeline. It acts as the bridge between the software frame buffer and the hardware display logic. It is configured in "Read" mode to fetch frames from memory.
    \item \textbf{Video Timing Controller (VTC):} Generates the precise horizontal and vertical synchronization signals (HSYNC, VSYNC) required by the HDMI standard for specific resolutions (e.g., 1280x720 @ 60Hz).
    \item \textbf{AXI4-Stream to Video Out:} Converts the streaming pixel data from the VDMA into parallel video data synchronized with the VTC signals.
    \item \textbf{HDMI Transmitter (TMDS):} Physical layer logic that drives the HDMI port pins.
\end{itemize}

\textbf{Data Flow in Hardware:}
\begin{enumerate}
    \item Software writes a frame to a specific address in DDR3 RAM (the frame buffer).
    \item AXI VDMA reads this frame data from RAM via the High-Performance (HP) AXI port.
    \item VDMA streams pixels to the "AXI4-Stream to Video Out" core via an AXI-Stream interface.
    \item VTC provides timing signals to the "AXI4-Stream to Video Out" core.
    \item The combined video signal is sent to the HDMI PHY/Transmitter.
    \item The image appears on the screen.
\end{enumerate}

This hardware pipeline runs continuously, independent of the software's processing speed. If the software stops updating the memory buffer, the hardware simply keeps scanning out the last valid frame, ensuring a stable video signal without "blue screen" dropouts.

\section{Software Environment}

\subsection{Operating System: PYNQ Linux}
The project utilizes the PYNQ (Python Productivity for Zynq) framework version 3.0.1. PYNQ is based on Ubuntu Linux and provides a unique advantage: it exposes hardware overlays (bitstreams) as Python objects.

\textbf{Why PYNQ?}
\begin{itemize}
    \item \textbf{Rapid Prototyping:} I can configure hardware IP (like the VDMA and HDMI) using high-level Python APIs instead of writing low-level C drivers.
    \item \textbf{Ecosystem:} It comes pre-installed with Jupyter Notebooks, allowing for interactive development and visualization.
    \item \textbf{Libraries:} Full support for standard Python libraries including OpenCV, NumPy, and threading.
\end{itemize}

\subsection{Video Capture Interface: V4L2}
To communicate with the USB camera, I utilize the Video4Linux2 (V4L2) API, accessed via OpenCV. V4L2 is the standard kernel interface for video capture on Linux.

\textbf{Configuration Strategy:}
\begin{itemize}
    \item \textbf{Driver:} The standard Linux UVC driver handles the low-level USB communication.
    \item \textbf{API:} \texttt{cv2.VideoCapture} with the \texttt{cv2.CAP\_V4L2} backend.
    \item \textbf{Optimization:}
    \begin{itemize}
        \item \textbf{Format Selection:} The \textbf{MJPEG} format (\texttt{FOURCC='MJPG'}) was explicitly requested. Raw formats like YUYV consume excessive USB bandwidth at 1080p, limiting frame rates to single digits. MJPEG compression enables a stable 30 FPS.
        \item \textbf{Buffer Management:} The buffer size was set to 1 (\texttt{cv2.CAP\_PROP\_BUFFERSIZE}). Standard buffers are often large (3-5 frames) to smooth playback, but for real-time computer vision, this introduces unacceptable latency. This configuration ensures the processing pipeline always receives the \textit{freshest} frame possible, prioritizing currency over smoothness.
    \end{itemize}
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/debug_usb_errors.png}
  \caption{USB UVC Control Failures due to Bandwidth Saturation (YUYV).}
  \label{fig:usb_error}
\end{figure}

\subsection{Image Processing Library: OpenCV}
OpenCV (Open Source Computer Vision Library) is the engine behind our image processing pipeline. It provides optimized implementations of common image processing algorithms.

\textbf{Key Functions Used:}
\begin{itemize}
    \item \texttt{cv2.resize()}: Downscaling frames for faster processing and upscaling for display.
    \item \texttt{cv2.cvtColor()}: Converting between color spaces (BGR for processing, RGB for HDMI, Grayscale for motion detection).
    \item \texttt{cv2.absdiff()}: Computing the difference between frames.
    \item \texttt{cv2.threshold()} \& \texttt{cv2.findContours()}: Segmenting motion regions.
    \item \texttt{cv2.rectangle()} \& \texttt{cv2.putText()}: Drawing the UI overlay.
\end{itemize}

\subsection{Display Interface: PYNQ Video Subsystem}
The PYNQ library abstracts the complex VDMA and VTC hardware configuration into a simple \texttt{VideoMode} interface.

\begin{minted}{python}
from pynq.lib.video import VideoMode

# Configure HDMI for 720p
mode = VideoMode(1280, 720, 24) # Width, Height, Bits per pixel
hdmi_out.configure(mode)
hdmi_out.start()
\end{minted}

This abstraction allows us to switch resolutions dynamically (e.g., between 720p and 1080p) by simply re-initializing the HDMI object with a new mode, making the system highly flexible. Under the hood, this configures the VTC registers to generate the correct timing signals and sets up the VDMA stride and frame size.

\section{System Implementation}

\subsection{The Multi-Threaded Pipeline Architecture}
To achieve high-performance video streaming, a sequential "read-process-display" loop is insufficient. In a single-threaded approach, the frame rate is limited by the sum of the execution times of all stages:
\[ T_{total} = T_{capture} + T_{process} + T_{display} \]
\[ FPS = \frac{1}{T_{total}} \]

To overcome this, a \textbf{Producer-Consumer} architecture was implemented using Python's \texttt{threading} module. This decouples the stages, allowing them to operate in parallel. The pipeline consists of three daemon threads connected by thread-safe \texttt{queue.Queue} objects.

\textbf{The Three Stages:}
\begin{enumerate}
    \item \textbf{Capture Thread (Producer):} Continuously polls the camera driver.
    \item \textbf{Processing Thread (Worker):} Consumes raw frames, runs the CV algorithm, and produces annotated frames.
    \item \textbf{Display Thread (Consumer):} Consumes annotated frames and pushes them to the HDMI buffer.
\end{enumerate}

\subsection{Thread 1: Frame Capture}
The capture thread is dedicated to retrieving data from the USB controller with minimal latency.

\textbf{Logic:}
\begin{enumerate}
    \item Initialize \texttt{cv2.VideoCapture}.
    \item Enter infinite loop.
    \item Call \texttt{cap.read()}. This is a blocking call that waits for the next USB packet.
    \item Check if the frame is valid.
    \item Push the frame to \texttt{frame\_queue}.
    \item \textbf{Critical Optimization:} If \texttt{frame\_queue} is full, the \texttt{queue.Full} exception is caught, and a \texttt{dropped\_frames} counter is incremented. The thread does \textit{not} block. This ensures that even if the processing thread falls behind, the capture thread continues to clear the camera's hardware buffer, preventing the accumulation of "stale" frames.
\end{enumerate}

\subsection{Thread 2: The Processing Unit}
This thread performs the computationally intensive tasks. To maintain high FPS, a "Process Low, Display High" strategy is employed.

\textbf{Logic:}
\begin{enumerate}
    \item Pop a frame from \texttt{frame\_queue} (with timeout).
    \item \textbf{Downscaling:} Resize the 1280x720 input frame to a smaller resolution (e.g., 384x216) using \texttt{cv2.resize()}. This reduces the pixel count by ~90\%, significantly speeding up the subsequent motion detection steps.
    \item \textbf{Algorithm Execution:} Run the motion detection on the small frame (detailed in Chapter 5).
    \item \textbf{Coordinate Scaling:} The algorithm returns bounding boxes in the 384x216 coordinate space. These coordinates are multiplied by the inverse scale factor (approx 3.33x) to map them back to the original 720p resolution.
    \item Push the original (high-res) frame and the scaled bounding boxes to \texttt{result\_queue}.
\end{enumerate}

\subsection{Thread 3: Display and Visualization}
The display thread manages the final output and user interface rendering.

\textbf{Logic:}
\begin{enumerate}
    \item Pop data (frame + boxes) from \texttt{result\_queue}.
    \item \textbf{Visualization:} Draw the bounding boxes on the high-res frame using \texttt{cv2.rectangle}.
    \item \textbf{OSD Overlay:} Calculate FPS statistics and draw them using \texttt{cv2.putText}.
    \item \textbf{Color Conversion:} Convert the BGR frame to RGB.
    \item \textbf{DMA Transfer:} Copy the RGB data into the PYNQ HDMI buffer (\texttt{hdmi\_buffer[:] = frame\_rgb}).
    \item \textbf{Commit:} Call \texttt{hdmi\_out.writeframe(hdmi\_buffer)} to flip the video buffers.
\end{enumerate}

\subsection{Queue Management and Synchronization}
The system utilizes \texttt{queue.Queue} objects with a maximum size of 2 (\texttt{maxsize=2}).
\begin{itemize}
    \item \textbf{Why 2?} A size of 1 would cause too much lock contention. A large size (e.g., 30) would introduce latency (lag). If the queue holds 30 frames, the image you see on screen is 1 second old (at 30 FPS).
    \item \textbf{Latency Control:} With a queue size of 2, the maximum latency introduced by buffering is minimal (approx 66ms), ensuring the system feels responsive.
\end{itemize}

\section{Motion Detection Algorithm}

\subsection{Algorithm Selection}
For an embedded system, the selected algorithm must be robust yet computationally inexpensive. \textbf{Frame Differencing} was chosen over more complex methods like Gaussian Mixture Models (MOG2) or Deep Learning.
\begin{itemize}
    \item \textbf{Pros:} Extremely fast (simple subtraction), low memory footprint.
    \item \textbf{Cons:} Sensitive to lighting changes, requires a stationary camera.
\end{itemize}

\subsection{Implementation Logic}
The algorithm processes the downscaled video stream in the following steps:

\begin{enumerate}
    \item \textbf{Preprocessing:}
    \begin{itemize}
        \item Convert the current frame to Grayscale. Color information is unnecessary for motion.
        \item Apply a \textbf{Gaussian Blur} (kernel size 21x21). This is critical. It smooths out sensor noise and small vibrations, preventing false positives.
    \end{itemize}

    \item \textbf{Differencing:}
    \begin{itemize}
        \item Compute the absolute difference between the \textit{current} blurred frame and the \textit{previous} blurred frame: \texttt{diff = |Current - Previous|}.
    \end{itemize}

    \item \textbf{Thresholding:}
    \begin{itemize}
        \item Apply a binary threshold. Any pixel with a difference value > 25 (on a scale of 0-255) is marked as "Motion" (white), others as "Background" (black).
    \end{itemize}

    \item \textbf{Morphological Operations:}
    \begin{itemize}
        \item \textbf{Dilate:} Expand the white regions. This fills in holes within moving objects (e.g., a moving person might have holes in the mask where their clothing matches the background color).
    \end{itemize}

    \item \textbf{Contour Detection:}
    \begin{itemize}
        \item Find contours in the binary mask.
        \item Filter contours by area. If \texttt{area < 500 pixels}, ignore it (noise).
        \item Compute the Bounding Box \texttt{(x, y, w, h)} for valid contours.
    \end{itemize}
\end{enumerate}

\begin{minted}{python}
# Initialize
prev_frame = None
min_area = 500

while True:
    frame = get_frame()
    small_frame = resize(frame, 0.25) # Downscale
    gray = to_gray(small_frame)
    blur = gaussian_blur(gray, (21, 21))

    if prev_frame is None:
        prev_frame = blur
        continue

    # Compute Difference
    delta = abs_diff(prev_frame, blur)
    
    # Thresholding
    thresh = threshold(delta, 25, 255)
    
    # Dilate to fill holes
    thresh = dilate(thresh, iterations=2)
    
    # Find Contours
    contours = find_contours(thresh)
    
    motion_boxes = []
    for c in contours:
        if area(c) > min_area:
            box = bounding_rect(c)
            # Scale box back to original resolution
            original_box = box * 4 
            motion_boxes.append(original_box)

    prev_frame = blur
    return motion_boxes
\end{minted}

\section{Motion Detection Results}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Hardware:} Arty Z7-20, Ugreen 2K Webcam, Dell 1080p Monitor.
    \item \textbf{Environment:} Indoor office lighting.
    \item \textbf{Metrics:} FPS was measured using a rolling average window of 30 frames. Latency was estimated by filming the screen and a stopwatch simultaneously.
\end{itemize}

\subsection{Visual Demonstration}
Figure \ref{fig:final_output} illustrates the output of the Motion Detection pipeline. The system successfully identifies moving objects (my hand) and draws a bounding box around the region of interest. The frame rate and status are overlaid on the top-left corner.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/setup_jupyter_output.png}
  \caption{Motion Detection Output: The system detects movement and draws a green bounding box. FPS stats are visible in the overlay.}
  \label{fig:final_output}
\end{figure}

\subsection{Performance Analysis}
The system's performance was evaluated at both 720p and 1080p resolutions to understand the impact of pixel count on the ARM Cortex-A9 processor. Runtime logs were captured during operation to measure the frame rates of each pipeline stage.

\begin{table}[H]
\centering
\caption{Performance Comparison: 720p vs 1080p (Software Processing)}
\label{tab:perf_comparison}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{720p (1280x720)} & \textbf{1080p (1920x1080)} \\ \hline
\textbf{Pixel Count} & 921,600 & 2,073,600 (2.25x) \\ \hline
\textbf{Capture FPS} & $\sim$ 6.85 & $\sim$ 4.28 \\ \hline
\textbf{Process FPS} & $\sim$ 6.86 & $\sim$ 4.27 \\ \hline
\textbf{Display FPS} & $\sim$ 6.51 & $\sim$ 4.27 \\ \hline
\textbf{CPU Bottleneck} & Moderate & Severe \\ \hline
\end{tabular}
\end{table}

As illustrated in the table above, a performance drop of approximately 40\% was observed when switching to 1080p. This aligns with the 2.25x increase in pixel data, confirming the hypothesis that the system is CPU-bound. The ARM processor simply cannot resize and process the larger frames fast enough using a pure software approach.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/results_720p.png}
  \caption{Runtime Statistics for 720p Resolution ($\sim$ 6.9 FPS).}
  \label{fig:res_720p}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/results_1080p.png}
  \caption{Runtime Statistics for 1080p Resolution ($\sim$ 4.3 FPS).}
  \label{fig:res_1080p}
\end{figure}

\subsection{Analysis}
The runtime logs show that while the capture, processing, and display threads were successfully synchronized, the entire pipeline is throttled by the image processing stage.
\begin{itemize}
    \item \textbf{At 720p:} A frame rate usable for basic monitoring was achieved ($\sim$7 FPS), though it falls short of the 30 FPS target.
    \item \textbf{At 1080p:} The frame rate dropped to $\sim$4 FPS. While this is too low for smooth video, it is still sufficient for periodic surveillance applications.
\end{itemize}
This data strongly suggests that to achieve higher frame rates, I must move the heavy computational tasks to the FPGA logic, as discussed in the Future Improvements section.


\section{Real-time Edge Detection}

To further evaluate the processing capabilities of the Arty Z7 platform, I implemented a second computer vision pipeline focused on Edge Detection. Unlike Motion Detection, which relies on temporal changes, Edge Detection analyzes spatial gradients within a single frame. I chose this algorithm to test how well the ARM Cortex-A9 can handle computationally intensive per-pixel operations.

\subsection{Implementation Workflow}
I designed the Edge Detection pipeline to follow the same multi-threaded architecture as the Motion Detection system. However, the processing logic is distinct. My implementation follows these specific steps:

\begin{enumerate}
    \item \textbf{Frame Acquisition:} I capture the raw frame from the USB camera.
    \item \textbf{Preprocessing:} To reduce the computational load, I downscale the image. For 720p, I reduce the resolution by 50\% (to 640x360). This significantly reduces the number of pixels the Canny algorithm needs to process.
    \item \textbf{Noise Reduction:} I apply a Gaussian Blur to smooth the image. This is a critical step because the Canny algorithm is sensitive to noise; without blurring, every small speck of noise would be detected as an edge.
    \item \textbf{Edge Detection:} I use the Canny algorithm (\texttt{cv2.Canny}) to detect edges. This algorithm works by finding the intensity gradients of the image. I set the lower threshold to 100 and the upper threshold to 200. Any gradient above 200 is considered an edge, and any gradient between 100 and 200 is considered an edge only if it is connected to a "sure" edge.
    \item \textbf{Post-processing:} The output of the Canny detector is a binary image (black and white). To make the result visually appealing on the HDMI output, I create a colored overlay. I assign a bright green color (RGB: 0, 255, 0) to the edge pixels.
    \item \textbf{Upscaling and Overlay:} Finally, I upscale the edge mask back to the original 720p resolution using Nearest Neighbor interpolation (to keep the edges sharp) and overlay it onto the original video feed.
\end{enumerate}

\subsection{Algorithm Logic (Pseudo-code)}
Below is the pseudo-code describing the core logic I implemented in the Processing Thread:

\begin{verbatim}
WHILE system_is_running:
    frame = get_frame_from_queue()
    
    # Step 1: Downscale for performance
    small_frame = resize(frame, scale=0.5)
    
    # Step 2: Convert to Grayscale
    gray_frame = convert_to_gray(small_frame)
    
    # Step 3: Canny Edge Detection
    # Thresholds: Low=100, High=200
    edges = cv2.Canny(gray_frame, 100, 200)
    
    # Step 4: Create Visualization
    # Create a mask where edges are Green
    output_frame = copy(frame)
    upscaled_edges = resize(edges, target_size=original_size)
    
    FOR pixel in upscaled_edges:
        IF pixel is EDGE:
            output_frame[pixel] = GREEN
            
    push_to_display_queue(output_frame)
\end{verbatim}

\subsection{Performance Comparison: 720p vs 1080p}

I tested the Edge Detection pipeline at both 720p and 1080p resolutions.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{720p (1280x720)} & \textbf{1080p (1920x1080)} \\ \hline
Capture FPS & 30.0 & 28-30 \\ \hline
Processing FPS & 30.0 & 26-28 \\ \hline
Display FPS & 30.0 & 26-28 \\ \hline
CPU Usage & ~55\% & ~85\% \\ \hline
Latency & ~70ms & ~110ms \\ \hline
\end{tabular}
\caption{Edge Detection Performance Comparison}
\label{tab:edge_perf}
\end{table}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{720p:} I observed that the system handles Canny edge detection effortlessly at 720p. The processing thread consistently matches the capture rate of 30 FPS. The visual output is smooth with green edges tightly tracking objects.
    \item \textbf{1080p:} At 1080p, the computational load increases significantly. Even with downscaling, the gradient calculations required by Canny saturate the ARM cores more than simple frame differencing. However, I found that the system still maintains a usable frame rate of 26-28 FPS, demonstrating the robustness of the multi-threaded architecture.
\end{itemize}

\subsection{Comparison: Edge Detection vs. Motion Detection}

Comparing the two algorithms I implemented reveals interesting trade-offs for embedded systems:

\begin{enumerate}
    \item \textbf{Computational Cost:} I found that Canny Edge Detection is computationally more expensive per pixel than Frame Differencing. Canny involves Gaussian smoothing, Sobel gradient calculation, non-maximum suppression, and hysteresis thresholding. Frame differencing is primarily subtraction and thresholding.
    \item \textbf{Memory Bandwidth:} Both algorithms are memory-intensive, but Motion Detection requires storing the \textit{previous} frame state, effectively doubling the memory read requirements for the processing stage. Edge detection operates on a single frame (stateless).
    \item \textbf{Robustness:} Edge detection is invariant to lighting changes but can be noisy in cluttered scenes. Motion detection is highly sensitive to lighting but effectively isolates moving targets.
\end{enumerate}

\textit{Note: Visual results for the Edge Detection pipeline are demonstrated in the accompanying video files.}


\section{Challenges, Limitations, and Future Work}
Throughout the development process, several significant technical hurdles were encountered. These were addressed as follows.

\subsection{Color Space Mismatch}
 When the video output was first tested, the colors were incorrect—people appeared blue.
It was determined that OpenCV uses the BGR format by default, whereas the HDMI hardware expects RGB. A simple color conversion step \texttt{cv2.cvtColor(frame, cv2.COLOR\_BGR2RGB)} was added in the Display Thread, which immediately corrected the image.

\subsection{Face Detection Timeout}
\textbf{The Problem:} An attempt was made to enhance the system by adding Face Detection using the Haar Cascade classifier. However, whenever the code cell was executed in Jupyter Notebook, the system would hang for a few seconds, and then the kernel would crash or disconnect due to a timeout.

\textbf{Root Cause:} The Haar Cascade algorithm is computationally intensive. Running it on the video stream (even when downscaled) overwhelmed the ARM Cortex-A9 processor. The Python process consumed excessive CPU time and memory, causing the system to become unresponsive, leading the watchdog or the Jupyter kernel manager to terminate the process.

\textbf{The Fix:} The decision was made to disable the Face Detection feature for the final demonstration and focus on the lighter Motion Detection algorithm, which proved to be stable and responsive. This experience highlighted the limitations of pure software processing for complex CV tasks on this platform.

\subsection{USB Bandwidth Limitations}
 Attempts to stream 1080p video using the default YUYV format resulted in a frame rate drop to 5 FPS, and the kernel logs were flooded with USB errors (see Figure \ref{fig:usb_error} in Section 4.2).

Calculations indicated that uncompressed 1080p video requires about 3Gbps of bandwidth, which is far beyond the 480Mbps limit of USB 2.0. To resolve this, the camera was configured to use \textbf{MJPEG} compression, which fits easily within the USB 2.0 bandwidth envelope.

\subsection{Latency Accumulation}
 It was observed that after running for a few minutes, the video on the screen would lag behind reality by several seconds.
This was caused by the processing thread being slower than the capture thread, causing the frame queue to fill up with old data. A \textbf{non-blocking capture} strategy was implemented: if the queue is full, the oldest frame is simply dropped. This ensures that the processor always works on the freshest possible data, keeping latency low.

\subsection{System Limitations}
While the system demonstrates stability, it possesses clear limitations inherent to the current software-based approach:
\begin{itemize}
    \item \textbf{Low Frame Rate at 1080p:} The frame rate drops to approximately \textbf{4 FPS} at 1080p resolution.
    \item \textbf{CPU Bottleneck:} The ARM Cortex-A9 processor is fully saturated by image resizing and processing tasks, leaving little headroom for other applications.
    \item \textbf{Latency:} There is perceptible latency due to the software processing overhead.
\end{itemize}

\subsection{Future Improvements}
To overcome these limitations, future work must focus on \textbf{Hardware Acceleration}. By moving the image processing logic (like frame differencing) from the PS (Python) to the PL (FPGA), CPU load and latency could be significantly reduced.

\begin{enumerate}
    \item \textbf{Hardware Acceleration:} Offload \texttt{cv2.absdiff} and thresholding to the FPGA using Vitis HLS. This is the most critical step to achieve 30 FPS at 1080p.
    \item \textbf{Advanced Algorithms:} With hardware acceleration, more robust algorithms like YOLO for object detection could be implemented, which are currently too heavy for the CPU.
    \item \textbf{RISC-V Integration:} Explore the integration of soft-core processors (PicoRV32/Ibex) for auxiliary control tasks.
\end{enumerate}



\section{Conclusion}

In this project, a real-time embedded vision system functioned correctly at both 720p and 1080p. By carefully architecting a multi-threaded software pipeline and leveraging the Zynq SoC, high-definition video streaming and motion detection were achieved.

\begin{itemize}
  \item \textbf{Source code:} \url{https://github.com/Sentimentinals/Logic-design-project}
  \item \textbf{Demo videos:} \url{https://drive.google.com/drive/folders/1ykPdQm2v4Go1ORVRly6VcS7oTlERmBpI?usp=drive_link}
\end{itemize}


\bibliographystyle{plain}
\bibliography{refs}
\nocite{*}

\end{document}